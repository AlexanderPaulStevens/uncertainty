{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7841fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "schematic overview of inference (regression)\n",
    "'''\n",
    "\n",
    "means_list, logvar_list =[], []\n",
    "for batch in range(int(np.ceil(X_range.shape[0] / self.batch_size))):\n",
    "    x_cat = X_cat[self.batch_size * batch: self.batch_size * (batch + 1)].cuda()\n",
    "    x_range = X_range[self.batch_size * batch: self.batch_size * (batch + 1)].cuda()\n",
    "    MC_samples = [self.predict_one(x_cat, x_range, stop_dropout=stop_dropout) for _ in range(K)]\n",
    "    means_list.append(torch.stack([tup[0] for tup in MC_samples]).view(K, x_range.shape[0]))\n",
    "    logvar_list.append(torch.stack([tup[1] for tup in MC_samples]).view(K, x_range.shape[0]))\n",
    "means = torch.cat([el for el in means_list], dim=1).cpu().data.numpy()\n",
    "logvar = torch.cat([el for el in logvar_list], dim=1).cpu().data.numpy()\n",
    "\n",
    "epistemic_uncertainty = np.var(means, 0)\n",
    "if heteroscedastic:\n",
    "    aleatoric_uncertainty = np.exp(logvar)  \n",
    "else:\n",
    "    aleatoric_uncertainty = np.zeros(logvar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514bd076",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "schematic overview of inference (classification)\n",
    "'''\n",
    "\n",
    "means_list, logvar_list =[], []\n",
    "for batch in range(int(np.ceil(X_range.shape[0] / self.batch_size))):\n",
    "    x_cat = X_cat[self.batch_size * batch: self.batch_size * (batch + 1)].cuda()\n",
    "    x_range = X_range[self.batch_size * batch: self.batch_size * (batch + 1)].cuda()\n",
    "    MC_samples = [self.predict_one(x_cat, x_range, stop_dropout) for _ in range(K)]\n",
    "    means_list.append(torch.stack([tup[0] for tup in MC_samples]).view(K, x_range.shape[0], self.C))\n",
    "    logvar_list.append(torch.stack([tup[1] for tup in MC_samples]).view(K, x_range.shape[0]))#.cpu().data.numpy())\n",
    "means = torch.cat([el for el in means_list], dim=1)\n",
    "logvar = torch.cat([el for el in logvar_list], dim=1).cpu().data.numpy()\n",
    "\n",
    "means = F.softmax(means, dim=2).cpu().data.numpy()\n",
    "predictive_entropy, mutual_information = calc_entropy(means)\n",
    "aleatoric_uncertainty = (predictive_entropy - mutual_information).cpu().data.numpy()\n",
    "epistemic_uncertainty = mutual_information.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b708d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy(mc_outputs):\n",
    "    '''\n",
    "    computes predictive entropy and mutual information\n",
    "    input 3-D Torch Tensor\n",
    "    Dimension 1: K Monte Carlo samples per case in validation set\n",
    "    Dimension 2: cases in validation set\n",
    "    Dimension3: C classes\n",
    "    Values should be in softmax along D3\n",
    "    '''\n",
    "    mc_outputs = torch.Tensor(mc_outputs)\n",
    "\n",
    "    mc_outputs_avg = torch.mean(mc_outputs, dim=0)\n",
    "    predictive_entropy = Categorical(probs=mc_outputs_avg).entropy()\n",
    "\n",
    "    entropy = Categorical(probs=mc_outputs).entropy()\n",
    "    expected_entropy = torch.mean(entropy, dim=0)\n",
    "\n",
    "    mutual_information = predictive_entropy-expected_entropy\n",
    "    return predictive_entropy, mutual_information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bayes",
   "language": "python",
   "name": "bayes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
